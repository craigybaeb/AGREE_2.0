# Import the required packages

# Explainers
import pathlib
import shap
import lime
import innvestigate # Smoothgrad, Vanilla Gradients, Input x Gradients, Layerwise Relevance Propagation, Guided Backpropagation, Deep Taylor
from shap import DeepExplainer, GradientExplainer, PermutationExplainer, KernelExplainer

# For data handling
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from imblearn.over_sampling import SMOTE # Used for correcting class imbalance
from collections import Counter # Will be used to show the class distribution
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from ucimlrepo import fetch_ucirepo # To import the dataset

# For training the Multi Layer Perceptron model
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Dense, Dropout)
from tensorflow.keras.callbacks import EarlyStopping

from utils.explain import Explanation
from utils.train import perform_outer_cross_validation
tf.compat.v1.disable_eager_execution() # We need to disable eager execution to work with iNNvestigate later

# For fitting a k-NN in the evaluation
from sklearn.neighbors import KNeighborsClassifier

# For evaluating the model
from sklearn.metrics import (
    precision_score,
    recall_score,
    f1_score,
    classification_report,
    accuracy_score,
    roc_auc_score
)

# For saving/fetching files from Google Drive
import pickle

# For visualisation
import seaborn as sns
import matplotlib.pyplot as plt

# To set the seed
import random

# Initialising SHAP to surpress warnings
shap.explainers._deep.deep_tf.op_handlers["SelectV2"] = shap.explainers._deep.deep_tf.passthrough
shap.explainers._deep.deep_tf.op_handlers["SplitV"] = shap.explainers._deep.deep_tf.passthrough

import os

def run_malware(seeds, num_splits_optimisation, seed, verbose, num_explanations, explainers_to_use):
    # Set constants
    filepath = 'experiments/results/malware'  # Filepath for saving data

    tf.keras.utils.set_random_seed(seed)

    # Create folders to save results
    pathlib.Path(filepath).mkdir(parents=True, exist_ok=True)
    pathlib.Path(f'{filepath}/training/results').mkdir(parents=True, exist_ok=True)
    pathlib.Path(f'{filepath}/training/figures').mkdir(parents=True, exist_ok=True)
    pathlib.Path(f'{filepath}/training/models').mkdir(parents=True, exist_ok=True)
    pathlib.Path(f'{filepath}/explanations/results').mkdir(parents=True, exist_ok=True)
    pathlib.Path(f'{filepath}/explanations/figures').mkdir(parents=True, exist_ok=True)
    pathlib.Path(f'{filepath}/results/results').mkdir(parents=True, exist_ok=True)
    pathlib.Path(f'{filepath}/results/figures').mkdir(parents=True, exist_ok=True)
    pathlib.Path(f'{filepath}/results/models').mkdir(parents=True, exist_ok=True)
    
    # Binary classification
    malware = 'experiments/data/malware.csv'

    # Load the data from GitHub
    malware_df = pd.read_csv(malware)

    # Re-name the dataset to plug-and-play with notebook template
    dataset = malware_df

    # Get the number of unique values in each column
    unique_counts = dataset.nunique()

    # Find the column names with non-unique values
    non_unique_columns = unique_counts[unique_counts > 1].index

    # Remove columns with non-unique values
    dataset = dataset[non_unique_columns]

    # Check for NaN values in the DataFrame
    nan_check = dataset.isna()

    # If you prefer to use pd.isnull(), you can do the following:
    nan_check = dataset.isnull()

    # Count the number of NaN values in each column
    nan_counts = nan_check.sum()

    dataset = dataset.dropna()

    x = dataset.drop(["Label"], axis=1)
    y = dataset["Label"]

    smote = SMOTE(sampling_strategy='auto', random_state=42)
    x_resampled, y_resampled = smote.fit_resample(x, y) 

    num_features = 199
    num_classes = 2

    # Get the class labels for the dataset
    class_labels = pd.unique(y_resampled)

    feature_names = [col for col in dataset.columns if col != 'Label']
    
    explain = Explanation(feature_names, class_labels, explainers_to_use, num_features, filepath)

    state_file = "experiments/results/malware/training/results/model_results_and_data.pkl"
    if os.path.exists(state_file):
        with open(state_file, 'rb') as file:
            saved_state = pickle.load(file)

    explanation_state_file = f'{filepath}/explanations/results/explanation_state.pkl'
    checkpoint = 0

    if os.path.exists(explanation_state_file):
        with open(explanation_state_file, 'rb') as file:
            explanation_state = pickle.load(file)
            checkpoint = explanation_state['checkpoint']


    perform_outer_cross_validation(x_resampled, y_resampled, num_classes, num_features, seeds, num_splits_optimisation, filepath)
    explain.get_explanations(x_resampled, y_resampled, seeds, filepath, saved_state['predictions'], checkpoint, num_explanations)


